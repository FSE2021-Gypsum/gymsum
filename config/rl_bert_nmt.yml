nmt:
  model_type: 'roberta'
  model_name_or_path: 'microsoft/codebert-base'
  config_name:
  tokenizer_name:
  do_lower_case: true
  data_workers: 0
  random_seed: 1013
  num_epochs: 2000
  batch_size: 32
  test_batch_size: 32
  include_ast: false
  lang: 'python'
  data_dir: 'data'
  src: 'code.original_subtoken'
  tgt: 'javadoc.original'
  model_dir: 'pretrain_models/code_bert_rl'
  model_name: 'code_bert_without_coverage'
  fix_rl: false
  rl_model_name: 'rl_extract1'
  checkpoint: true
  rl_pretrained:
  pretrained:
  max_examples: -1
  uncase: true
  max_characters_per_token: 30
  valid_metric: 'bleu'
  display_iter: 25
  sort_by_len: true
  only_test: false
  print_copy_info: false
  print_one_target: false
  log_file:
  cuda: true
  parallel: false
  model:
    max_len: 60
    beam_size: 1
    torchscript: false
    use_code_type: false
    code_tag_type: 'subtoken'
    max_source_length: 300
    max_target_length: 32
    emsize: 768
    share_decoder_embeddings: true
    model_type: transformer
    num_head: 12
    d_k: 64
    d_v: 64
    d_ff: 2048
    src_pos_emb: false
    tgt_pos_emb: true
    max_relative_pos: 0
    use_neg_dist: true
    nlayers: 6
    trans_drop: 0.2
    dropout_emb: 0.2
    dropout: 0.2
    copy_attn: true
    early_stop: 20
    warmup_steps: 0
    warmup_epochs: 0
    optimizer: adam
    learning_rate: 0.00005
    lr_decay: 0.99
    use_all_enc_layers: false
    nhid: 200
    bidirection: true
    layer_wise_attn: false
    n_characters: 260
    char_emsize: 16
    filter_size: 5
    nfilters: 100
    attn_type: general
    coverage_attn: false
    review_attn: false
    force_copy: false
    reuse_copy_attn: false
    split_decoder: false
    reload_decoder_state:
    conditional_decoding: false
    grad_clipping: 1.0
    weight_decay: 0
    momentum: 0

    code2seq:
      remove: false
      data:
        home: code2seq/data/
        dict: java.dict.c2s
        train: java.train.raw.txt.json
        dev: java.val.raw.txt.json
        test: java.test.raw.txt.json
      hyper:
        vocab_size_sub_token:
        vocab_size_nodes:
        vocab_size_target:
        token_size: 128
        hidden_size: 64
        num_layers: 4
        bidirectional: True
        rnn_dropout: 0.5
        embeddings_dropout: 0.3
        num_k: 200

  extract:
    embed_size: 400
    bid: true
    n_layers: 4
    hidden_size: 400
    dropout: 0.2
    dim: [ 400 ]
    optimizer: adam
    learning_rate: 0.00005
    lr_decay: 0.99
    grad_clipping: 1.0
    weight_decay: 0
    momentum: 0
    warmup_steps: 0
    warmup_epochs: 0
    early_stop: 20