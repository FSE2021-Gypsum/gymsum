nmt:
  data_workers: 5
  random_seed: 1013
  num_epochs: 200
  batch_size: 32
  test_batch_size: 64
  dataset_name: [ 'python' ]
  model_dir: 'pretrain_models/transformer'
  model_name: '20201125-cc56dc57.mdl.checkpoint'
  data_dir: 'data/'
  train_src: [ 'train/code.original_subtoken' ]
  train_src_tag: [ ]
  train_tgt: [ 'train/javadoc.original' ]
  dev_src: [ 'dev/code.original_subtoken' ]
  dev_src_tag: [ ]
  dev_tgt: [ 'dev/javadoc.original' ]
  checkpoint: true
  pretrained:
  max_examples: -1
  uncase: true
  src_vocab_size: 50000
  tgt_vocab_size: 30000
  max_characters_per_token: 30
  valid_metric: 'bleu'
  display_iter: 25
  sort_by_len: true
  only_test: false
  print_copy_info: false
  print_one_target: false
  log_file:
  cuda: true
  parallel: false
  model:
    use_code_type: false
    code_tag_type: 'subtoken'
    use_src_word: true
    use_src_char: false
    use_tgt_word: true
    use_tgt_char: false
    max_src_len: 400
    max_tgt_len: 30
    emsize: 512
    fix_embeddings: false
    src_vocab_size: 50000
    tgt_vocab_size: 30000
    share_decoder_embeddings: true
    model_type: transformer
    num_head: 8
    d_k: 64
    d_v: 64
    d_ff: 2048
    src_pos_emb: false
    tgt_pos_emb: true
    max_relative_pos: [32]
    use_neg_dist: true
    nlayers: 6
    trans_drop: 0.2
    dropout_emb: 0.2
    dropout: 0.2
    copy_attn: true
    early_stop: 20
    warmup_steps: 0
    warmup_epochs: 0
    optimizer: adam
    learning_rate: 0.0001
    lr_decay: 0.99
    use_all_enc_layers: false
    rnn_type: LSTM
    nhid: 200
    bidirection: true
    layer_wise_attn: false
    n_characters: 260
    char_emsize: 16
    filter_size: 5
    nfilters: 100
    attn_type: general
    coverage_attn: false
    review_attn: false
    force_copy: false
    reuse_copy_attn: false
    split_decoder: false
    reload_decoder_state:
    conditional_decoding: false
    dropout_rnn: 0.2
    grad_clipping: 5.0
    weight_decay: 0
    momentum: 0

