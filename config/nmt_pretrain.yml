nmt:
  mode: train
  vocab: 'data/python/vocab.code2nl.bin'
  save_to: 'pretrain_models/code2nl.pretrain'
  task: 'code2nl'
  log_every: 500
  valid_niter: 500
  valid_metric: 'bleu'
  save_model_after: 2
  beam_size: 10
  batch_size: 32
  hidden_size: 512
  embed_size: 512
  uniform_init: 0.1
  dropout: 0.2
  clip_grad: 5.0
  decode_max_time_step: 50
  lr_decay: 0.8
  lr: 0.002
  train_src: 'data/python/train/code.original_subtoken'
  train_tgt: 'data/python/train/javadoc.original'
  dev_src: 'data/python/dev/code.original_subtoken'
  dev_tgt: 'data/python/dev/javadoc.original'
  test_src: 'data/python/test/code.original_subtoken'
  test_tgt: 'data/python/test/javadoc.original'
  sample_size: 10
  eval_dir:
  load_model: 'pretrain_models/code2nl.pretrain.iter2000.bin'
  save_to_file:
  save_nbest: false
  patience: 50
  max_niter: -1
  smooth_bleu: false
  sample_method: 'random'